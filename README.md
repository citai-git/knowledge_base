

# Welcome to CitAI Papers knowledge base

##### Table of Contents  
[FEP](#fep)  \
[Associative learning](#assoclearn) \
[AGI](#agi) \
[Category theory](#cat) \
[Reinforcement learning](#rl) \
[Deep learning](#dl) \
[Medical imagining](#medical) \
[Statistics](#stats) \
[Neurips](#stats) 
    


<a name="fep"/>

## FEP



### Action and Perception as Divergence Minimization

**url** : [https://arxiv.org/abs/2009.01791?s=09](https://arxiv.org/abs/2009.01791?s=09)

**abstract**:  We introduce a unified objective for action and perception of intelligent agents. Extending representation learning and control, we minimize the joint divergence between the world and a target distribution. Intuitively, such agents use perception to align their beliefs with the world, and use actions to align the world with their beliefs. Minimizing the joint divergence to an expressive target maximizes the mutual information between the agent's representations and inputs, thus inferring representations that are informative of past inputs and exploring future inputs that are informative of the representations. This lets us derive intrinsic objectives, such as representation learning, information gain, empowerment, and skill discovery from minimal assumptions. Moreover, interpreting the target distribution as a latent variable model suggests expressive world models as a path toward highly adaptive agents that seek large niches in their environments, while rendering task rewards optional. The presented framework provides a common language for comparing a wide range of objectives, facilitates understanding of latent variables for decision making, and offers a recipe for designing novel objectives. We recommend deriving future agent objectives from the joint divergence to facilitate comparison, to point out the agent's target distribution, and to identify the intrinsic objective terms needed to reach that distribution.

<a name="assoclearn"/>

## Associative learning


<a name="agi"/>

## AGI

<a name="cat"/>

## Category theory

<a name="rl"/>

## Reinforcement learning

<a name="dl"/>

## Deep learning

<a name="medical"/>

## Medical imagining

<a name="stats"/>

## Statistics


<a name="neurips2020"/>

## NEURIPS 2020 papers

### Graphs
    

 - #### CASTLE: Regularization via Auxiliary Causal Graph Discovery
 https://proceedings.neurips.cc/paper/2020/hash/1068bceb19323fe72b2b344ccf85c254-Abstract.html
 - #### Graphon Neural Networks and the Transferability of Graph Neural Networks
https://proceedings.neurips.cc/paper/2020/hash/12bcd658ef0a540cabc36cdf2b1046fd-Abstract.html
- ####   Pointer graph networks 
https://proceedings.neurips.cc/paper/2020/hash/176bf6219855a6eb1f3a30903e34b6fb-Abstract.html
- #### Multimodal Graph Networks for Compositional Generalization in Visual Question Answering
https://proceedings.neurips.cc/paper/2020/hash/1fd6c4e41e2c6a6b092eb13ee72bce95-Abstract.html

- #### Learning Dynamic Belief Graphs to Generalize on Text-Based Games
https://proceedings.neurips.cc/paper/2020/hash/1fc30b9d4319760b04fab735fbfed9a9-Abstract.html

- #### Learning Graph Structure With A Finite-State Automaton Layer
https://proceedings.neurips.cc/paper/2020/hash/1fdc0ee9d95c71d73df82ac8f0721459-Abstract.html

- #### A Graph Similarity for Deep Learning 
https://proceedings.neurips.cc/paper/2020/file/0004d0b59e19461ff126e3a08a814c33-Paper.pdf

- #### Neural Message Passing for Multi-Relational Ordered and Recursive Hypergraphs
https://proceedings.neurips.cc/paper/2020/hash/217eedd1ba8c592db97d0dbe54c7adfc-Abstract.html

- #### How hard is to distinguish graphs with graph neural networks?
https://proceedings.neurips.cc/paper/2020/hash/23685a2431acad7789c1e3d43ea1522c-Abstract.html

- #### Natural Graph Networks
https://proceedings.neurips.cc/paper/2020/hash/2517756c5a9be6ac007fe9bb7fb92611-Abstract.html

- #### Towards Deeper Graph Neural Networks with Differentiable Group Normalization

https://proceedings.neurips.cc/paper/2020/hash/33dd6dba1d56e826aac1cbf23cdcca87-Abstract.html

- #### Towards practical differentially private causal graph discovery
https://proceedings.neurips.cc/paper/2020/hash/3b13b1eb44b05f57735764786fab9c2c-Abstract.html

- #### Graph Meta Learning via Local Subgraphs
https://proceedings.neurips.cc/paper/2020/hash/412604be30f701b1b1e3124c252065e6-Abstract.html

### Transfer learning

 - #### A Combinatorial Perspective on Transfer Learning
 https://proceedings.neurips.cc/paper/2020/hash/0a3b6f64f0523984e51323fe53b8c504-Abstract.html
 - #### What is being transferred in transfer learning?
 https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html
